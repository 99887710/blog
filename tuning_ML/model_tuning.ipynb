{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Explanation of notebook\nThe goal of this notebook is to explain the tuning factors (or regularization factors) of common regression models. \n\nFor a deeper explanation of the models themselves, please check out this Coursera course: https://www.coursera.org/learn/python-machine-learning. The content of this notebook is derived from lecture 2 in that course."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib notebook\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import LinearSVC\n\n",
      "execution_count": 111,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## KNN (works both for classification and regression)\nA KNN model has a single factor, namely **n_neighbors**.\n* If 1: perfect fit for training data, doesn't generalize well\n* If >1 <n_trainingset: non-perfect fit for training data, generalizes better, better assumed result for testing dataset\n* If close to or equal n_trainingset: underfit, all results = most common case in training dataset (or average)\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_to_classify, classes = make_classification( n_samples = 250, random_state = 0)\n\nX_train, X_test, y_train, y_test = train_test_split(data_to_classify, classes, random_state=0)\n\n\nfor k in [1,5,10,50,100,X_train.shape[0]]:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    print('k {}\\t: Training accuracy = {:.2f},\\tTest accuracy = {:.2f}'\n         .format(k,knn.score(X_train, y_train),knn.score(X_test, y_test)))\n\n",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "k 1\t: Training accuracy = 1.00,\tTest accuracy = 0.68\nk 5\t: Training accuracy = 0.89,\tTest accuracy = 0.81\nk 10\t: Training accuracy = 0.84,\tTest accuracy = 0.79\nk 50\t: Training accuracy = 0.86,\tTest accuracy = 0.83\nk 100\t: Training accuracy = 0.84,\tTest accuracy = 0.81\nk 187\t: Training accuracy = 0.51,\tTest accuracy = 0.51\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Ridge regression\nFormula: $Ridge(w,b) = \\sum \\limits _{i=1} ^{n} (y_{i} - (w \\cdot x_{i} + b))^2 + \\alpha \\sum \\limits _{j=1} ^{p} w_{j}^2 $\n\nExplanation: A ridge regression is a linear regression, that adds a penalty for large w parameters. As you can see, a Ridge model uses the squared $w$ factors. This means $\\alpha$ is a L2 factor.\n\nRegularization factor = $\\alpha$ :\n* If $\\ alpha $ = 0: Regular leased squared\n* Low values of $\\ alpha $: Model overfit. Good results for training data, less for testing data.\n* Growing $\\ alpha $: Simpler model. The higher $\\ alpha $ gets, the more tuning occurs, brining $w$ factors closer to zero. Likely a optimum point for best test dataset performance"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_regression, y = make_regression()\nX,y = make_regression(n_features=25,\n        bias=20,\n        n_informative=8,\n        noise = 100)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nfor alpha in [0,1,10,25,50,100,250,1000]:\n    linRidge = Ridge(alpha = alpha).fit(X_train,y_train)\n    sum_w = np.sum(linRidge.coef_)\n    zero_w = np.sum(linRidge.coef_ == 0)\n    r2_train = linRidge.score(X_train, y_train)\n    r2_test = linRidge.score(X_test, y_test)\n    print(\"alpha = {}, \\t Sum of W = {:.0f}  \\t W is 0 = {} \\t r2 train = {:.3f} \\t r2 test = {:.3f}\".format(alpha,sum_w,zero_w,r2_train,r2_test))\n    ",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": "alpha = 0, \t Sum of W = 510  \t W is 0 = 0 \t r2 train = 0.841 \t r2 test = 0.604\nalpha = 1, \t Sum of W = 494  \t W is 0 = 0 \t r2 train = 0.840 \t r2 test = 0.633\nalpha = 10, \t Sum of W = 398  \t W is 0 = 0 \t r2 train = 0.809 \t r2 test = 0.701\nalpha = 25, \t Sum of W = 312  \t W is 0 = 0 \t r2 train = 0.740 \t r2 test = 0.651\nalpha = 50, \t Sum of W = 237  \t W is 0 = 0 \t r2 train = 0.643 \t r2 test = 0.550\nalpha = 100, \t Sum of W = 163  \t W is 0 = 0 \t r2 train = 0.507 \t r2 test = 0.413\nalpha = 250, \t Sum of W = 87  \t W is 0 = 0 \t r2 train = 0.311 \t r2 test = 0.236\nalpha = 1000, \t Sum of W = 26  \t W is 0 = 0 \t r2 train = 0.106 \t r2 test = 0.072\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Lasso regression\nFormula: $Lasso(w,b) = \\sum \\limits _{i=1} ^{n} (y_{i} - (w \\cdot x_{i} + b))^2 + \\alpha \\sum \\limits _{j=1} ^{p} |w_{j}| $\n\nExplanation: A lasso regression is a linear regression, that adds a penalty for large w parameters. Compared to Ridge, Lasso uses the absolute value of the coefficient. This has the effect that a lot of $w$ factors will be zero. $\\alpha$ in Lasso is an L1 regularization factor.\n\nRegularization factor = $\\alpha$ :\n* If $\\ alpha $ = 0: Regular leased squared\n* Low values of $\\ alpha $: Model overfit. Good results for training data, less for testing data.\n* Growing $\\ alpha $: Simpler model. The higher $\\ alpha $ gets, the more tuning occurs, making more $w$ become zero. Likely a optimum point for best test dataset performance."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_regression, y = make_regression()\nX,y = make_regression(n_features=25,\n        bias=20,\n        n_informative=8,\n        noise = 100)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nfor alpha in [0,1,10,25,50,100,250,1000]:\n    linLasso = Lasso(alpha = alpha).fit(X_train,y_train)\n    sum_w = np.sum(linLasso.coef_)\n    zero_w = np.sum(linLasso.coef_ == 0)\n    r2_train = linLasso.score(X_train, y_train)\n    r2_test = linLasso.score(X_test, y_test)\n    print(\"alpha = {}, \\t Sum of W = {:.0f}  \\t W is 0 = {} \\t r2 train = {:.3f} \\t r2 test = {:.3f}\".format(alpha,sum_w,zero_w,r2_train,r2_test))\n    ",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": "alpha = 0, \t Sum of W = 653  \t W is 0 = 0 \t r2 train = 0.923 \t r2 test = 0.663\nalpha = 1, \t Sum of W = 636  \t W is 0 = 0 \t r2 train = 0.923 \t r2 test = 0.689\nalpha = 10, \t Sum of W = 525  \t W is 0 = 12 \t r2 train = 0.893 \t r2 test = 0.729\nalpha = 25, \t Sum of W = 403  \t W is 0 = 17 \t r2 train = 0.816 \t r2 test = 0.639\nalpha = 50, \t Sum of W = 228  \t W is 0 = 19 \t r2 train = 0.606 \t r2 test = 0.388\nalpha = 100, \t Sum of W = 53  \t W is 0 = 22 \t r2 train = 0.198 \t r2 test = -0.042\nalpha = 250, \t Sum of W = 0  \t W is 0 = 25 \t r2 train = 0.000 \t r2 test = -0.177\nalpha = 1000, \t Sum of W = 0  \t W is 0 = 25 \t r2 train = 0.000 \t r2 test = -0.177\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:478: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  positive)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n  ConvergenceWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Polynomial regression\nTransforms x to x', with x' being all polynomial combinations of any original two features. Example for degree = 2 :\n\n$ x = (x_{1}, x_{2}) \\Rightarrow x' = (x_{1}, x_{2}, x_{1}^2,x_{1}x_{2}, x_{2}^2) \\\\\n\\hat y = \\hat w_{1}x_{1} + \\hat w_{2}x_{2} + \\hat w_{11}x_{1}^2 + \\hat w_{12}x_{1}x_{2} + \\hat w_{22}x_{2}^2\n$\n\nThe tuning factor of a polynomial regression is the **degree**. The degree refers to how many variables ($x_{i}$) should be combined into a new feature.\nA polynomial regression isn't different from any other regression, since you update the featurespace, not the regression algorithm itself. Meaning, you could still apply a Ridge/Lasso after applying a polynomial. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Logistic regression\nFirst up: logistic regression isn't actually a regression model, but a classification model. It takes input features and outputs a numbers between 0 and 1 to classify a certain outcome. \n\nFormula: $ \\hat y = logistic(\\hat b + \\hat w_{1}x_{1} + \\dots +  w_{n}x_{n}) \\\\\n=  \\frac{1}{1+ \\mathrm{e}^{-(\\hat b + \\hat w_{1}x_{1} + \\dots +  w_{n}x_{n}) }}$\n\nRegularization factor = $C$. This factor is an L2 factor, like a Ridge model (meaning, it uses the squared factors $w_i$).\n\n- If C very close to zero, the factors of the logistic regression become very large. This has the impact that the model could behave poorly for both the training and test dataset.\n- Increasing C will decrease the factors $w$, which might lead to less overfitting and a better result. A optimum likely exists between 0 and $\\infty$\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_to_classify, classes = make_classification(n_samples = 250, n_features=20, n_informative=5,\n                                n_clusters_per_class=1, flip_y = 0.1,random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(data_to_classify, classes, random_state=0)\n\n\nfor C in [0.001,0.01,0.1,1,10,100,1000]:\n    logreg = LogisticRegression(C=C).fit(X_train, y_train)\n   \n    print('C {}\\t: Training accuracy = {:.2f},\\tTest accuracy = {:.2f}'\n         .format(C,logreg.score(X_train, y_train),logreg.score(X_test, y_test)))\n",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": "C 0.001\t: Training accuracy = 0.86,\tTest accuracy = 0.92\nC 0.01\t: Training accuracy = 0.88,\tTest accuracy = 0.94\nC 0.1\t: Training accuracy = 0.92,\tTest accuracy = 0.94\nC 1\t: Training accuracy = 0.92,\tTest accuracy = 0.92\nC 10\t: Training accuracy = 0.91,\tTest accuracy = 0.90\nC 100\t: Training accuracy = 0.90,\tTest accuracy = 0.90\nC 1000\t: Training accuracy = 0.90,\tTest accuracy = 0.90\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Linear support vector machine\nA LSVM is a classifier. It fits a linear line in a dataset to seperate it in two groups.\nA key property of a LSVM is the margin of error that it uses for calculating the best fitting classifier. The margin of error is how much distance we tolerate on any point  from the classifier. A higher margin of error means thus that points closer to the classifier would be 'ignored' in favor of better generalization.\n\nFormula: $ \\hat y = f(x,w,b) = sign(w \\cdot x + b)\n$\n\nRegularization factor: $C$.\nCounter-intuitive: Larger $C$ means _less_ regularization, meaning more chance for overfitting. Smaller $C$ means _more_ regularization.\nC is corelated with the margin of error. A larger value of C tries to fit the training data as well as possible, meaning the margin of error would be smaller. If C is smaller, the model would try to generalize better, meaning a larger margin of error would be tolerated.\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_to_classify, classes = make_classification(n_samples = 250, n_features=20, n_informative=5,\n                                n_clusters_per_class=1, flip_y = 0.1,random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(data_to_classify, classes, random_state=0)\n\n\nfor C in [0.001,0.01,0.1,1,10,100,1000,10000,100000]:\n    svc = LinearSVC(C=C).fit(X_train, y_train)\n   \n    print('C {}\\t: Training accuracy = {:.2f},\\tTest accuracy = {:.2f}'\n         .format(C,svc.score(X_train, y_train),svc.score(X_test, y_test)))\n",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": "C 0.001\t: Training accuracy = 0.89,\tTest accuracy = 0.94\nC 0.01\t: Training accuracy = 0.91,\tTest accuracy = 0.94\nC 0.1\t: Training accuracy = 0.93,\tTest accuracy = 0.94\nC 1\t: Training accuracy = 0.91,\tTest accuracy = 0.92\nC 10\t: Training accuracy = 0.92,\tTest accuracy = 0.94\nC 100\t: Training accuracy = 0.90,\tTest accuracy = 0.92\nC 1000\t: Training accuracy = 0.89,\tTest accuracy = 0.92\nC 10000\t: Training accuracy = 0.89,\tTest accuracy = 0.90\nC 100000\t: Training accuracy = 0.90,\tTest accuracy = 0.94\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Kernel SVM - Radial Basis Function Kernel\nA Kernel SVM is a SVM which applies a transformation on the input data. One of those transformations is the Radial Basis Function. \n\nFormula: $ RBF(x,x') = exp(- \\gamma \\cdot d(x,x')^2 $ with $d(x,x')$ being the Euclidian distance.\n\nParameters: $\\gamma$ and $C$.\nC is the same as above, namely the regularization factor.\n- Bigger C means less regularization, \n- smaller C means more regularization.\n\n\n$\\gamma$ is the function of the RBF to determine the effect of a single training point on others around it. \n- A higher gamma means less influence on farther points.\n- A lower gamma means more influence on farther points.\n\nIn other words.\n\n- A higher gamma means **tighter** decision boundaries.\n- A lower gamma means **broader** decision boundaries.\n\nA good example of showing the relationship between C and $\\gamma$ is shown here: https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Decision trees\nA decision tree is another classifier model. It tries to learn repeatable steps to navigate through a dataset, aka rules. \n\nThere are three parameters to tune a decision tree:\n* **max_depth**: Controls the maximum depth of the tree.\n* **min_samples_leaf**: Is the minimum amount of samples in each leaf.\n* **max_leaf_nodes**: The maximum amount of leaves (aka edges) in the tree."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
